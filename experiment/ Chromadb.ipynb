{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "import chromadb\n",
    "\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "\n",
    "\n",
    "# load the documents from the PDF directory\n",
    "loader = PyPDFDirectoryLoader(\"./pdfs\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Next step is embedding \n",
    "\n",
    "# Creating database\n",
    "\n",
    "\n",
    "# stirong in a directory and operating search operaions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d39dfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Embeddings...\n",
      "Loading PDFs from ./pdfs ...\n",
      "Loaded 15 pages.\n",
      "Split into 52 chunks.\n",
      "Creating/Updating ChromaDB...\n",
      "‚úÖ Database created and saved locally.\n",
      "\n",
      "üîç Searching for: 'What is the summary of the document?'\n",
      "\n",
      "--- Search Results ---\n",
      "\n",
      "[Result 1]\n",
      "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "corpus of english: The penn treebank. Computational linguistics, 19(2):313‚Äì330, 1993.\n",
      "[26] David McCl...\n",
      "\n",
      "[Result 2]\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as ...\n",
      "\n",
      "[Result 3]\n",
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
      "model. All metrics are on the English-to-German translation development set, newstest2013. Liste...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain Imports\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# 1. LOAD KEYS\n",
    "load_dotenv()\n",
    "\n",
    "hf_api_key = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "if not hf_api_key:\n",
    "    raise ValueError(\"Error: HUGGINGFACEHUB_API_TOKEN is missing from .env file.\")\n",
    "\n",
    "# 2. DEFINE EMBEDDING CLASS (Same Robust Class as before)\n",
    "class RobustHuggingFaceEmbeddings(Embeddings):\n",
    "    def __init__(self, api_key, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.client = InferenceClient(token=api_key)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        embeddings_list = []\n",
    "        for text in texts:\n",
    "            try:\n",
    "                response = self.client.feature_extraction(text, model=self.model_name)\n",
    "                if isinstance(response, np.ndarray):\n",
    "                    if response.ndim == 2:\n",
    "                        embedding = np.mean(response, axis=0).tolist()\n",
    "                    elif response.ndim == 1:\n",
    "                        embedding = response.tolist()\n",
    "                    embeddings_list.append(embedding)\n",
    "                elif isinstance(response, list):\n",
    "                    if len(response) > 0 and isinstance(response[0], list):\n",
    "                         embedding = np.mean(response, axis=0).tolist()\n",
    "                    else:\n",
    "                         embedding = response\n",
    "                    embeddings_list.append(embedding)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                raise\n",
    "        return embeddings_list\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        result = self.embed_documents([text])\n",
    "        return result[0]\n",
    "\n",
    "print(\"Connecting to Embeddings...\")\n",
    "embeddings = RobustHuggingFaceEmbeddings(api_key=hf_api_key)\n",
    "\n",
    "# 3. LOAD & SPLIT DOCUMENTS\n",
    "print(\"Loading PDFs from ./pdfs ...\")\n",
    "loader = PyPDFDirectoryLoader(\".././pdfs\")\n",
    "documents = loader.load()\n",
    "\n",
    "if not documents:\n",
    "    print(\"Warning: No documents found.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(documents)} pages.\")\n",
    "    \n",
    "    # Split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    final_documents = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(final_documents)} chunks.\")\n",
    "\n",
    "    # 4. CREATE CHROMADB (LOCAL VECTOR STORE)\n",
    "    # We persist the data to a folder named 'db' so we don't have to reload every time\n",
    "    persist_directory = \"./chroma_db\"\n",
    "    \n",
    "    print(\"Creating/Updating ChromaDB...\")\n",
    "    \n",
    "    # This automatically:\n",
    "    # 1. Embeds the documents using our custom class\n",
    "    # 2. Stores them in the './chroma_db' folder\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=final_documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Database created and saved locally.\")\n",
    "\n",
    "    # 5. PERFORM SIMILARITY SEARCH\n",
    "    query = \"What is the summary of the document?\"\n",
    "    print(f\"\\nüîç Searching for: '{query}'\")\n",
    "    \n",
    "    # Get top 3 matches\n",
    "    results = vectordb.similarity_search(query, k=3)\n",
    "    \n",
    "    print(\"\\n--- Search Results ---\")\n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"\\n[Result {i+1}]\")\n",
    "        print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# retrieval QA system with ChromaDB and HuggingFace Embeddings complete.\n",
    "# call grok api clien and create chain , retrival qa from chain types\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
